# Pollution-Autoencoders

## Overview
<p><i>Pollution Autoencoders</i> uses a deep autoencoder neural network to characterize cities in the United States based on time-series readings of various polluting gases and particulates. 

We compared the [Autoencoders](https://www.deeplearningbook.org/contents/autoencoders.html) method with [Principle Component Analysis (PCA)](https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186), an older technique for reducing the dimensionality of a data set.

The main advantage of Autoencoders have over PCA is in their capability of discovering non-linear relationships between polluting gases. Autoencoders are able to capture more of the explained variance because of this increased flexibility.

We measured the explained variance across 190 dimensions for each of the 8 polluting gases by performing a simple linear regression on the compressed data generated by both the PCA and Autoencoder methods. As a control, we also performed linear regression on the uncompressed data.</p>
<br>

## Setup Guide 
<p>
Clone the repository to the desired location.
<pre><code>git clone https://github.com/nshan651/Pollution-Autoencoders.git</code></pre>
Install the required packages.
<pre><code>pip install pandas numpy matplotlib sklearn tensorflow requests</code></pre>
</p>
<br>

## A Brief Introduction to Autoencoders

An autoencoder is a neural network that encodes its input data into a low-dimensional latent space encoding. Once the input is encoded, it can be decoded by reconstructing the input using the latent space.

<center>
<img title="ae architecture" alt="ae architecture" height=50% width = 40% src=images/pipeline/ae_architecture.png>
</center>

Autoencoders are usually restricted in ways that allow them to copy approximately. which forces the model to prioritize which elements of the input should be copied. In the process of imperfectly copying the input, we hope that the latent dimensions will take on useful properties. Such a method of constraining the hidden layer to have smaller dimensions than the input is referred to as an <b> Undercomplete Autoencoder</b>.

For example, a particularly interesting usecase for us was in clustering cities based on the first two dimensions. The first two dimensions of both models were especially useful because they happened to result in some of the largest jumps in explained variance.

<br>

## Tools & Pipeline

<center>
<img title="Pipeline" alt="Pipeline" height=100% width = 100% src=images/pipeline/pipeline.png>
</center>

<br>

## Methodology

<br>

## Results

<br>

## Conclusion

<br>

## Sources
